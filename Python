import mediapipe as mp
import cv2
import numpy as np
from mediapipe.framework.formats import landmark_pb2
import time
from math import sqrt
import win32api
import pyautogui
 
 
 #reconnaitre les 20 points geometriques de la main
mp_drawing = mp.solutions.drawing_utils
mp_hands = mp.solutions.hands
click=0
 #prendre l'entrée de capture webcam
video = cv2.VideoCapture(0) 


 #appeler la classe mediapipe.hands.Hands avec 2 parametres 
 #les donnée sont stockés ds la variable frame avec la classe .read de openCV  
 #changer le format de l'image 
with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.8) as hands: 
    while video.isOpened():
        _, frame = video.read()
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
         
        image = cv2.flip(image, 1)
 
        imageHeight, imageWidth, _ = image.shape
 #la sortie est stockés dans la variable results 
        results = hands.process(image)
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) 
        
        #verifier s'il y a des multi hand landmarks disponibles 
        #DrawingSpec : classe  dessiner les points de reperes et les connecter 
        
  
        if results.multi_hand_landmarks:
            for num, hand in enumerate(results.multi_hand_landmarks):
                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS, 
                                        mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2),
                                         ) 

       #les coordonnées de l'index et de pouce 
#itérer les points 
#convertir les valeurs de types de données speciaux en chaine 
        if results.multi_hand_landmarks != None: #verifier s'il y a une main dans le cadre 
          for handLandmarks in results.multi_hand_landmarks:
            for point in mp_hands.HandLandmark:
 
    
                normalizedLandmark = handLandmarks.landmark[point]
                pixelCoordinatesLandmark = mp_drawing._normalized_to_pixel_coordinates(normalizedLandmark.x, normalizedLandmark.y, imageWidth, imageHeight)
    
                point=str(point) 

#stocker les valeurs de l'index et de pouce 
#utiliser win32api pour deplacer le curseur 
 
                if point=='HandLandmark.INDEX_FINGER_TIP':
                 try:
                    indexfingertip_x=pixelCoordinatesLandmark[0]
                    indexfingertip_y=pixelCoordinatesLandmark[1]
                    win32api.SetCursorPos((indexfingertip_x*4,indexfingertip_y*5))
 
                 except:
                    pass
 
                elif point=='HandLandmark.THUMB_TIP': 

                
                 try:
                    thumbfingertip_x=pixelCoordinatesLandmark[0]
                    thumbfingertip_y=pixelCoordinatesLandmark[1]
                    #print("thumb",thumbfingertip_x)
 
                 except:
                    pass

 # automatioser le click 
 #calcul de la distance entre les coordonnées 
 #utiliser pyautogui.click pour cliquer 

                try:
                    #pyautogui.moveTo(indexfingertip_x,indexfingertip_y)
                    Distance_x= sqrt((indexfingertip_x-thumbfingertip_x)**2 + (indexfingertip_x-thumbfingertip_x)**2)
                    Distance_y= sqrt((indexfingertip_y-thumbfingertip_y)**2 + (indexfingertip_y-thumbfingertip_y)**2)
                    if Distance_x<5 or Distance_x<-5:
                        if Distance_y<5 or Distance_y<-5:
                            click=click+1
                            if click%5==0:
                                print("single click")
                                pyautogui.click()                            
 
                except:
                    pass 

                #afficher le flux de sortie 
                #utiliser la methode imshow() de opencv 
                #appuie sur la touche q pour sortie 
 
        cv2.imshow('Hand Tracking', image)
 
        if cv2.waitKey(10) & 0xFF == ord('q'):
            break
 
video.release()
